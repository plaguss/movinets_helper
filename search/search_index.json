{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"MoViNets helper This package contains some code gathered while dealing with MoViNets . If you read through the movinet_tutorial.ipynb but feel like you could get some extra help to use the model for your own data, this may be useful. Table Of Contents The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework and consists of four separate parts: Installation How-To Guides API reference 3.1. utils 3.2. modelling 3.3. reader 3.4. writer","title":"Intro"},{"location":"#movinets-helper","text":"This package contains some code gathered while dealing with MoViNets . If you read through the movinet_tutorial.ipynb but feel like you could get some extra help to use the model for your own data, this may be useful.","title":"MoViNets helper"},{"location":"#table-of-contents","text":"The documentation follows the best practice for project documentation as described by Daniele Procida in the Di\u00e1taxis documentation framework and consists of four separate parts: Installation How-To Guides API reference 3.1. utils 3.2. modelling 3.3. reader 3.4. writer","title":"Table Of Contents"},{"location":"how-to-guides/","text":"How to create a dataset. It is assumed the videos are in mp4 format, but should be similar for other formats. Assuming the videos/clips are stored with the following layout: clips/ label_a/ label_a_1.mp4 label_a_2.mp4 ... label_b/ label_b_1.mp4 label_b_2.mp4 ... ... Keep the labels in a .txt. label_a label_b ... Grab the paths to the videos and extract the labels: import glob from pathlib import Path from movinets_helper.utils import get_labels glob_videos = glob . glob ( str ( Path ( clips_path ) / \"**/*.mp4\" )) video_paths = [ Path ( p ) for p in glob_videos ] labels = get_labels ( video_paths ) Create a dataset with the paths, labels and classes: import pandas as pd from movinets_helper.utils import create_class_map class_map = create_class_map ( \"labels.txt\" ) dataset_df = pd . DataFrame ({ \"labels\" : labels , \"files\" : glob_videos }) dataset_df [ \"classes\" ] = dataset_df [ \"labels\" ] . map ( class_map ) Split the dataset in train and test prior to the TFRecords generation. from movinets_helper.utils import split_train_test train_dataset_df , test_dataset_df = split_train_test ( dataset_df , train_size = 0.8 ) We are ready to generate the dataset. For the number of videos to store per record, see tfrecord guide . Regarding the number of frames and the resolution, it will be user dependent. import movinets_helper.writer as wr wr . convert_mp4_to_tfrecord ( train_dataset_df [[ \"classes\" , \"files\" ]], \"path-for-training\" , n_videos_in_record : int = 25 , n_frames_per_video : int = 10 , resolution : int = 224 , ) wr . convert_mp4_to_tfrecord ( test_dataset_df [[ \"classes\" , \"files\" ]], \"path-for-testing\" , n_videos_in_record : int = 25 , n_frames_per_video : int = 10 , resolution : int = 224 , ) Note : To create a dataset of approximately 540 videos, with shapes (10, 224, 224, 3), took close to 4 minutes. Keep in mind, the result files will be compressed with gzip, but will take up a lot of space. How to ingest a dataset. Access to the dataset generated. from pathlib import Path from movinets_helper.reader import get_dataset , format_features dataset_dir = \"path-to-files\" train_dataset_dir = dataset_dir / \"train\" test_dataset_dir = dataset_dir / \"test\" ds_train = get_dataset ( list ( Path ( train_dataset_dir ) . iterdir ())) ds_test = get_dataset ( list ( Path ( test_dataset_dir ) . iterdir ())) To make it usable for the model selected, the inputs must be formatted appropriately and batched. batch_size = 8 # From the tutorial ds_train = ds_train . map ( format_features , num_parallel_calls = tf . data . AUTOTUNE ) . batch ( batch_size ) ds_train = ds_train . repeat () ds_train = ds_train . prefetch ( 2 ) ds_test = ds_test . map ( format_features , num_parallel_calls = tf . data . AUTOTUNE , deterministic = True ) . batch ( batch_size ) ds_test = ds_test . repeat () ds_test = ds_test . prefetch ( 2 ) The function format_features is by default set to the resolution of a0 model, it may be updated in the following way, and according to your number of classes: from functools import partial format_features_a2 = partial ( format_features , resolution = 224 , num_classes = 9 ) Fine-Tuning Movinet A2 Base This package has been used to fine-tune the model on google colab and the version dependencies are adapted for this case. The movinet_tutorial.ipynb uses tensorflow versions 2.9 and higher, and the correct movinet model versions are defined there, but there may be some error when calling fit to train the model. In that case, take a look at this issue First load the pretrained weights of the chosen model !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q !tar -xvf movinet_a0_base.tar.gz Get the parameters expected for the model. Read the docs for more info on this. import movinet_helper.modelling as model cfg = model . ConfigMovinetA2Base ( epochs = EPOCHS ) Where EPOCHS corresponds to the number of epochs sent to .fit . Assuming you have the info of the dataset in a csv of the following form, compute the train and test steps for the model with the following code snipet. Otherwise, just estimate the length of the training and test datasets to be used: from movinets_helper.utils import get_number_of_steps import pandas as pd train_dataset_df = pd . read_csv ( < your_dir > / \"train_dataset_df.csv\" ) test_dataset_df = pd . read_csv ( < your_dir > / \"test_dataset_df.csv\" ) train_steps , total_train_steps = get_number_of_steps ( len ( train_dataset_df ), batch_size , epochs = config . epochs ) test_steps , _ = get_number_of_steps ( len ( test_dataset_df ), batch_size , epochs = config . epochs ) To get the hyperparameters, the following function loads all of them (there is no info regarding the number of training total number of training steps, so this parameter must be given by the anyway): params = modeling . default_hyperparams ( total_train_steps ) Get your model and compile it with the parameters loaded: model = modeling . make_model ( NUM_CLASSES , config ) model . compile ( loss = params [ \"loss_function\" ], optimizer = params [ \"optimizer\" ], metrics = params [ \"metrics\" ]) There is a bug with the callbacks, which may be obtained from the default_hyperparams , but currently they must be loaded separated. These should be tailored to your needs. checkpoint_filepath = str ( < your_dir > / \"movinet_a2_base_checkpoints\" ) model_checkpoint_callback = tf . keras . callbacks . ModelCheckpoint ( filepath = checkpoint_filepath , save_weights_only = True , monitor = 'val_top_1' , mode = 'max' , save_best_only = True ) callbacks = [ tf . keras . callbacks . TensorBoard (), model_checkpoint_callback ] Call fit on the model and hope for your data to be kind :) results = model . fit ( ds_train , validation_data = ds_test , epochs = config . epochs , steps_per_epoch = train_steps , validation_steps = test_steps , callbacks = callbacks , validation_freq = 1 , verbose = 1 ) model . save ( str ( < your_dir > / \"movinet_base_a0_fine_tuned\" )) Loading your trained model Once the model has been fine tuned, it can be loaded to return predictions. Even though the model would be saved as a SavedModel , there are some bugs to load the model directly using the api model.load , but the following piece (which can be found in the tutorial) does the job: from movinet_helper.utils import load_model model = load_model ( \"<path_to_model>\" ) Your model is ready to be tested!","title":"How To Guide"},{"location":"how-to-guides/#how-to-create-a-dataset","text":"It is assumed the videos are in mp4 format, but should be similar for other formats. Assuming the videos/clips are stored with the following layout: clips/ label_a/ label_a_1.mp4 label_a_2.mp4 ... label_b/ label_b_1.mp4 label_b_2.mp4 ... ... Keep the labels in a .txt. label_a label_b ... Grab the paths to the videos and extract the labels: import glob from pathlib import Path from movinets_helper.utils import get_labels glob_videos = glob . glob ( str ( Path ( clips_path ) / \"**/*.mp4\" )) video_paths = [ Path ( p ) for p in glob_videos ] labels = get_labels ( video_paths ) Create a dataset with the paths, labels and classes: import pandas as pd from movinets_helper.utils import create_class_map class_map = create_class_map ( \"labels.txt\" ) dataset_df = pd . DataFrame ({ \"labels\" : labels , \"files\" : glob_videos }) dataset_df [ \"classes\" ] = dataset_df [ \"labels\" ] . map ( class_map ) Split the dataset in train and test prior to the TFRecords generation. from movinets_helper.utils import split_train_test train_dataset_df , test_dataset_df = split_train_test ( dataset_df , train_size = 0.8 ) We are ready to generate the dataset. For the number of videos to store per record, see tfrecord guide . Regarding the number of frames and the resolution, it will be user dependent. import movinets_helper.writer as wr wr . convert_mp4_to_tfrecord ( train_dataset_df [[ \"classes\" , \"files\" ]], \"path-for-training\" , n_videos_in_record : int = 25 , n_frames_per_video : int = 10 , resolution : int = 224 , ) wr . convert_mp4_to_tfrecord ( test_dataset_df [[ \"classes\" , \"files\" ]], \"path-for-testing\" , n_videos_in_record : int = 25 , n_frames_per_video : int = 10 , resolution : int = 224 , ) Note : To create a dataset of approximately 540 videos, with shapes (10, 224, 224, 3), took close to 4 minutes. Keep in mind, the result files will be compressed with gzip, but will take up a lot of space.","title":"How to create a dataset."},{"location":"how-to-guides/#how-to-ingest-a-dataset","text":"Access to the dataset generated. from pathlib import Path from movinets_helper.reader import get_dataset , format_features dataset_dir = \"path-to-files\" train_dataset_dir = dataset_dir / \"train\" test_dataset_dir = dataset_dir / \"test\" ds_train = get_dataset ( list ( Path ( train_dataset_dir ) . iterdir ())) ds_test = get_dataset ( list ( Path ( test_dataset_dir ) . iterdir ())) To make it usable for the model selected, the inputs must be formatted appropriately and batched. batch_size = 8 # From the tutorial ds_train = ds_train . map ( format_features , num_parallel_calls = tf . data . AUTOTUNE ) . batch ( batch_size ) ds_train = ds_train . repeat () ds_train = ds_train . prefetch ( 2 ) ds_test = ds_test . map ( format_features , num_parallel_calls = tf . data . AUTOTUNE , deterministic = True ) . batch ( batch_size ) ds_test = ds_test . repeat () ds_test = ds_test . prefetch ( 2 ) The function format_features is by default set to the resolution of a0 model, it may be updated in the following way, and according to your number of classes: from functools import partial format_features_a2 = partial ( format_features , resolution = 224 , num_classes = 9 )","title":"How to ingest a dataset."},{"location":"how-to-guides/#fine-tuning-movinet-a2-base","text":"This package has been used to fine-tune the model on google colab and the version dependencies are adapted for this case. The movinet_tutorial.ipynb uses tensorflow versions 2.9 and higher, and the correct movinet model versions are defined there, but there may be some error when calling fit to train the model. In that case, take a look at this issue First load the pretrained weights of the chosen model !wget https://storage.googleapis.com/tf_model_garden/vision/movinet/movinet_a0_base.tar.gz -O movinet_a0_base.tar.gz -q !tar -xvf movinet_a0_base.tar.gz Get the parameters expected for the model. Read the docs for more info on this. import movinet_helper.modelling as model cfg = model . ConfigMovinetA2Base ( epochs = EPOCHS ) Where EPOCHS corresponds to the number of epochs sent to .fit . Assuming you have the info of the dataset in a csv of the following form, compute the train and test steps for the model with the following code snipet. Otherwise, just estimate the length of the training and test datasets to be used: from movinets_helper.utils import get_number_of_steps import pandas as pd train_dataset_df = pd . read_csv ( < your_dir > / \"train_dataset_df.csv\" ) test_dataset_df = pd . read_csv ( < your_dir > / \"test_dataset_df.csv\" ) train_steps , total_train_steps = get_number_of_steps ( len ( train_dataset_df ), batch_size , epochs = config . epochs ) test_steps , _ = get_number_of_steps ( len ( test_dataset_df ), batch_size , epochs = config . epochs ) To get the hyperparameters, the following function loads all of them (there is no info regarding the number of training total number of training steps, so this parameter must be given by the anyway): params = modeling . default_hyperparams ( total_train_steps ) Get your model and compile it with the parameters loaded: model = modeling . make_model ( NUM_CLASSES , config ) model . compile ( loss = params [ \"loss_function\" ], optimizer = params [ \"optimizer\" ], metrics = params [ \"metrics\" ]) There is a bug with the callbacks, which may be obtained from the default_hyperparams , but currently they must be loaded separated. These should be tailored to your needs. checkpoint_filepath = str ( < your_dir > / \"movinet_a2_base_checkpoints\" ) model_checkpoint_callback = tf . keras . callbacks . ModelCheckpoint ( filepath = checkpoint_filepath , save_weights_only = True , monitor = 'val_top_1' , mode = 'max' , save_best_only = True ) callbacks = [ tf . keras . callbacks . TensorBoard (), model_checkpoint_callback ] Call fit on the model and hope for your data to be kind :) results = model . fit ( ds_train , validation_data = ds_test , epochs = config . epochs , steps_per_epoch = train_steps , validation_steps = test_steps , callbacks = callbacks , validation_freq = 1 , verbose = 1 ) model . save ( str ( < your_dir > / \"movinet_base_a0_fine_tuned\" ))","title":"Fine-Tuning Movinet A2 Base"},{"location":"how-to-guides/#loading-your-trained-model","text":"Once the model has been fine tuned, it can be loaded to return predictions. Even though the model would be saved as a SavedModel , there are some bugs to load the model directly using the api model.load , but the following piece (which can be found in the tutorial) does the job: from movinet_helper.utils import load_model model = load_model ( \"<path_to_model>\" ) Your model is ready to be tested!","title":"Loading your trained model"},{"location":"installation/","text":"General Installation If you are going to train the model using Google Colab, it may be better to skip this section and go directly to the next one. Otherwise, to install movinets_helper , just pip install it: pip install movinets_helper Installing on Google Colab When dealing with Google Colab, the preferred way to install this package is to just install the dependencies needed, and after this, install the package without the dependencies. The dependencies should be the same to the movinet_tutorial.ipynb offered by the authors (keep in mind this commands are rin in the notebook): !python -m pip install --upgrade pip !pip install -q tf-models-official !pip install tensorflow-io !pip uninstall -q -y opencv-python-headless !pip install -q \"opencv-python-headless<4.3\" !pip install tensorflow == 2 .8 !apt install --allow-change-held-packages libcudnn8 = 8 .1.0.77-1+cuda11.2 After the correct installation of these dependencies, install the package: pip install --no-deps movinets_helper","title":"Installation"},{"location":"installation/#general-installation","text":"If you are going to train the model using Google Colab, it may be better to skip this section and go directly to the next one. Otherwise, to install movinets_helper , just pip install it: pip install movinets_helper","title":"General Installation"},{"location":"installation/#installing-on-google-colab","text":"When dealing with Google Colab, the preferred way to install this package is to just install the dependencies needed, and after this, install the package without the dependencies. The dependencies should be the same to the movinet_tutorial.ipynb offered by the authors (keep in mind this commands are rin in the notebook): !python -m pip install --upgrade pip !pip install -q tf-models-official !pip install tensorflow-io !pip uninstall -q -y opencv-python-headless !pip install -q \"opencv-python-headless<4.3\" !pip install tensorflow == 2 .8 !apt install --allow-change-held-packages libcudnn8 = 8 .1.0.77-1+cuda11.2 After the correct installation of these dependencies, install the package: pip install --no-deps movinets_helper","title":"Installing on Google Colab"},{"location":"api/modeling/","text":"This section contains the documentation of movinets_helper\\modeling.py . Call the models in a single function ready to be trained. These are just a bunch of wrapper functions on the Movinet model. BaseConfig dataclass Configuration for the data ingested to the model. Source code in movinets_helper/modeling.py 15 16 17 18 19 20 21 22 23 @dataclass class BaseConfig : \"\"\"Configuration for the data ingested to the model.\"\"\" num_frames : int = 10 resolution : int = 172 batch_size : int = 8 channels : int = 3 epochs : int = 3 version : str = \"base\" build_classifier ( backbone , num_classes , model_config , freeze_backbone = True , stream = False ) Builds a classifier on top of a backbone model. Copied from movinet_tutorial.ipynb. Wraps the backbone with a new classifier to crete a new classifier head with num_classes outputs. Freezes all layers but the last. Parameters: Name Type Description Default backbone movinet . Movinet description required num_classes int Number of classes for your given model. required model_config BaseConfig Configuration of the model chosen. Contains required freeze_backbone int Whether to freeze all the layers but the last. Defaults to True. True stream bool Use base or stream model. Only implemented base, the parameter is here for future use. Defaults to False. False Returns: Name Type Description _type_ Model ready to be compiled. Source code in movinets_helper/modeling.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def build_classifier ( backbone : movinet . Movinet , num_classes : int , model_config : BaseConfig , freeze_backbone : int = True , stream : bool = False , ): \"\"\"Builds a classifier on top of a backbone model. Copied from movinet_tutorial.ipynb. Wraps the backbone with a new classifier to crete a new classifier head with num_classes outputs. Freezes all layers but the last. Args: backbone (movinet.Movinet): _description_ num_classes (int): Number of classes for your given model. model_config (BaseConfig): Configuration of the model chosen. Contains freeze_backbone (int, optional): Whether to freeze all the layers but the last. Defaults to True. stream (bool, optional): Use base or stream model. Only implemented base, the parameter is here for future use. Defaults to False. Returns: _type_: Model ready to be compiled. \"\"\" model = movinet_model . MovinetClassifier ( backbone = backbone , num_classes = num_classes , output_states = stream ) model . build ( [ model_config . batch_size , model_config . num_frames , model_config . resolution , model_config . resolution , model_config . channels , ] ) # Freeze all layers but the last for fine-tuning if freeze_backbone : for layer in model . layers [: - 1 ]: layer . trainable = False model . layers [ - 1 ] . trainable = True return model default_hyperparams ( total_train_steps ) Gathers the hyperparameters used in the movinet tutorial to train the models. Just call this if you aren't willing to investigate further. FIXME Explain how to use it Set examples Source code in movinets_helper/modeling.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def default_hyperparams ( total_train_steps : int ): \"\"\"Gathers the hyperparameters used in the movinet tutorial to train the models. Just call this if you aren't willing to investigate further. FIXME: - Explain how to use it - Set examples \"\"\" return { \"loss_function\" : loss_function (), \"metrics\" : metrics (), \"optimizer\" : optimizer ( total_train_steps ), \"callbacks\" : loss_function (), }","title":"Modeling"},{"location":"api/modeling/#movinets_helper.modeling.BaseConfig","text":"Configuration for the data ingested to the model. Source code in movinets_helper/modeling.py 15 16 17 18 19 20 21 22 23 @dataclass class BaseConfig : \"\"\"Configuration for the data ingested to the model.\"\"\" num_frames : int = 10 resolution : int = 172 batch_size : int = 8 channels : int = 3 epochs : int = 3 version : str = \"base\"","title":"BaseConfig"},{"location":"api/modeling/#movinets_helper.modeling.build_classifier","text":"Builds a classifier on top of a backbone model. Copied from movinet_tutorial.ipynb. Wraps the backbone with a new classifier to crete a new classifier head with num_classes outputs. Freezes all layers but the last. Parameters: Name Type Description Default backbone movinet . Movinet description required num_classes int Number of classes for your given model. required model_config BaseConfig Configuration of the model chosen. Contains required freeze_backbone int Whether to freeze all the layers but the last. Defaults to True. True stream bool Use base or stream model. Only implemented base, the parameter is here for future use. Defaults to False. False Returns: Name Type Description _type_ Model ready to be compiled. Source code in movinets_helper/modeling.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def build_classifier ( backbone : movinet . Movinet , num_classes : int , model_config : BaseConfig , freeze_backbone : int = True , stream : bool = False , ): \"\"\"Builds a classifier on top of a backbone model. Copied from movinet_tutorial.ipynb. Wraps the backbone with a new classifier to crete a new classifier head with num_classes outputs. Freezes all layers but the last. Args: backbone (movinet.Movinet): _description_ num_classes (int): Number of classes for your given model. model_config (BaseConfig): Configuration of the model chosen. Contains freeze_backbone (int, optional): Whether to freeze all the layers but the last. Defaults to True. stream (bool, optional): Use base or stream model. Only implemented base, the parameter is here for future use. Defaults to False. Returns: _type_: Model ready to be compiled. \"\"\" model = movinet_model . MovinetClassifier ( backbone = backbone , num_classes = num_classes , output_states = stream ) model . build ( [ model_config . batch_size , model_config . num_frames , model_config . resolution , model_config . resolution , model_config . channels , ] ) # Freeze all layers but the last for fine-tuning if freeze_backbone : for layer in model . layers [: - 1 ]: layer . trainable = False model . layers [ - 1 ] . trainable = True return model","title":"build_classifier()"},{"location":"api/modeling/#movinets_helper.modeling.default_hyperparams","text":"Gathers the hyperparameters used in the movinet tutorial to train the models. Just call this if you aren't willing to investigate further. FIXME Explain how to use it Set examples Source code in movinets_helper/modeling.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def default_hyperparams ( total_train_steps : int ): \"\"\"Gathers the hyperparameters used in the movinet tutorial to train the models. Just call this if you aren't willing to investigate further. FIXME: - Explain how to use it - Set examples \"\"\" return { \"loss_function\" : loss_function (), \"metrics\" : metrics (), \"optimizer\" : optimizer ( total_train_steps ), \"callbacks\" : loss_function (), }","title":"default_hyperparams()"},{"location":"api/reader/","text":"This section contains the documentation of movinets_helper\\reader.py . Functionalities to read a TFRecordDataset ready to train a network. add_states ( video , label , stream_states = {}) This function is expected to modify the dataset to make it ready for the movinet stream models, but couldn't get to train them Parameters: Name Type Description Default video _type_ description required label _type_ description required stream_states dict description . Defaults to {}. {} Returns: Type Description Tuple [ Dict [ str , tf . Tensor ], tf . Tensor ] Tuple[Dict[str, tf.Tensor], tf.Tensor]: description Source code in movinets_helper/reader.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def add_states ( video , label , stream_states = {} ) -> Tuple [ Dict [ str , tf . Tensor ], tf . Tensor ]: \"\"\"This function is expected to modify the dataset to make it ready for the movinet stream models, but couldn't get to train them Args: video (_type_): _description_ label (_type_): _description_ stream_states (dict, optional): _description_. Defaults to {}. Returns: Tuple[Dict[str, tf.Tensor], tf.Tensor]: _description_ \"\"\" return { ** stream_states , \"image\" : video }, label encode_label ( label , num_classes ) One hot encodes the labels according to the number of classes. Parameters: Name Type Description Default label str Label representing the movement of the video. required num_classes int Total number of classes in the dataset. required Returns: Type Description tf . Tensor tf.Tensor: Encoded representation of the label Source code in movinets_helper/reader.py 8 9 10 11 12 13 14 15 16 17 18 def encode_label ( label : str , num_classes : int ) -> tf . Tensor : \"\"\"One hot encodes the labels according to the number of classes. Args: label (str): Label representing the movement of the video. num_classes (int): Total number of classes in the dataset. Returns: tf.Tensor: Encoded representation of the label \"\"\" return tf . one_hot ( label , num_classes ) format_features ( video , label , resolution = 172 , scaling_factor = 255.0 , num_classes = 2 ) Transforms the data to have the appropriate shape. This function must be called on a tf.data.Dataset (passed via its .map method). Parameters: Name Type Description Default video tf . Tensor Decoded video. required label str Corresponding class of the video. required resolution int The resolution will be model dependent. Movinet a0 and a1 use 172, a2 uses 224. Defaults to 172. 172 scaling_factor float Given the videos have the pixels in the range 0.255, transforms the data to the range [0, 1]. Defaults to 255.. 255.0 num_classes int Number of classes the model is trained on. I.e. for Kinetics 600 will be 600, for UCF 101 that will be the number. Defaults to 2. 2 Returns: Type Description Tuple [ tf . Tensor , tf . Tensor ] Tuple[tf.Tensor, tf.Tensor]: When iterated, the first element will be the video, and the second will be the label as required by the model. Source code in movinets_helper/reader.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def format_features ( video : tf . Tensor , label : str , resolution : int = 172 , scaling_factor : float = 255.0 , num_classes : int = 2 , ) -> Tuple [ tf . Tensor , tf . Tensor ]: \"\"\"Transforms the data to have the appropriate shape. This function must be called on a tf.data.Dataset (passed via its .map method). Args: video (tf.Tensor): Decoded video. label (str): Corresponding class of the video. resolution (int, optional): The resolution will be model dependent. Movinet a0 and a1 use 172, a2 uses 224. Defaults to 172. scaling_factor (float, optional): Given the videos have the pixels in the range 0.255, transforms the data to the range [0, 1]. Defaults to 255.. num_classes (int, optional): Number of classes the model is trained on. I.e. for Kinetics 600 will be 600, for UCF 101 that will be the number. Defaults to 2. Returns: Tuple[tf.Tensor, tf.Tensor]: When iterated, the first element will be the video, and the second will be the label as required by the model. \"\"\" label = tf . cast ( label , tf . int32 ) label = encode_label ( label , num_classes ) video = tf . image . resize ( video , ( resolution , resolution )) video = tf . cast ( video , tf . float32 ) / scaling_factor return video , label get_dataset ( filenames ) Generates a td.data.Dataset from the TFRecord files. This is the appropriate format to be passed to model.fit, after it is formated and there is some batch called, so the final video object ingested by the model will have the shape [n_videos, n_frames, resolution, resolution, channels]. Parameters: Name Type Description Default filenames List [ str ] List of .tfrecord files. required Returns: Type Description tf . data . Dataset tf.data.Dataset: Dataset ready to train the model. Example target_path is the path to the .tfrecords files directory. ds = get_dataset(list(Path(target_path).iterdir())) This iterable may be formatted appropriately: ds = get_dataset(list(Path(target_path_train).iterdir())) ds = ds.map(format_features) To see a single example: next(iter(ds)) Source code in movinets_helper/reader.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def get_dataset ( filenames : List [ str ]) -> tf . data . Dataset : \"\"\"Generates a td.data.Dataset from the TFRecord files. This is the appropriate format to be passed to model.fit, after it is formated and there is some batch called, so the final video object ingested by the model will have the shape [n_videos, n_frames, resolution, resolution, channels]. Args: filenames (List[str]): List of .tfrecord files. Returns: tf.data.Dataset: Dataset ready to train the model. Example: target_path is the path to the .tfrecords files directory. >>> ds = get_dataset(list(Path(target_path).iterdir())) This iterable may be formatted appropriately: >>> ds = get_dataset(list(Path(target_path_train).iterdir())) >>> ds = ds.map(format_features) To see a single example: >>> next(iter(ds)) \"\"\" raw_dataset = tf . data . TFRecordDataset ( filenames , compression_type = \"GZIP\" ) return raw_dataset . map ( _parse_example )","title":"reader"},{"location":"api/reader/#movinets_helper.reader.add_states","text":"This function is expected to modify the dataset to make it ready for the movinet stream models, but couldn't get to train them Parameters: Name Type Description Default video _type_ description required label _type_ description required stream_states dict description . Defaults to {}. {} Returns: Type Description Tuple [ Dict [ str , tf . Tensor ], tf . Tensor ] Tuple[Dict[str, tf.Tensor], tf.Tensor]: description Source code in movinets_helper/reader.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def add_states ( video , label , stream_states = {} ) -> Tuple [ Dict [ str , tf . Tensor ], tf . Tensor ]: \"\"\"This function is expected to modify the dataset to make it ready for the movinet stream models, but couldn't get to train them Args: video (_type_): _description_ label (_type_): _description_ stream_states (dict, optional): _description_. Defaults to {}. Returns: Tuple[Dict[str, tf.Tensor], tf.Tensor]: _description_ \"\"\" return { ** stream_states , \"image\" : video }, label","title":"add_states()"},{"location":"api/reader/#movinets_helper.reader.encode_label","text":"One hot encodes the labels according to the number of classes. Parameters: Name Type Description Default label str Label representing the movement of the video. required num_classes int Total number of classes in the dataset. required Returns: Type Description tf . Tensor tf.Tensor: Encoded representation of the label Source code in movinets_helper/reader.py 8 9 10 11 12 13 14 15 16 17 18 def encode_label ( label : str , num_classes : int ) -> tf . Tensor : \"\"\"One hot encodes the labels according to the number of classes. Args: label (str): Label representing the movement of the video. num_classes (int): Total number of classes in the dataset. Returns: tf.Tensor: Encoded representation of the label \"\"\" return tf . one_hot ( label , num_classes )","title":"encode_label()"},{"location":"api/reader/#movinets_helper.reader.format_features","text":"Transforms the data to have the appropriate shape. This function must be called on a tf.data.Dataset (passed via its .map method). Parameters: Name Type Description Default video tf . Tensor Decoded video. required label str Corresponding class of the video. required resolution int The resolution will be model dependent. Movinet a0 and a1 use 172, a2 uses 224. Defaults to 172. 172 scaling_factor float Given the videos have the pixels in the range 0.255, transforms the data to the range [0, 1]. Defaults to 255.. 255.0 num_classes int Number of classes the model is trained on. I.e. for Kinetics 600 will be 600, for UCF 101 that will be the number. Defaults to 2. 2 Returns: Type Description Tuple [ tf . Tensor , tf . Tensor ] Tuple[tf.Tensor, tf.Tensor]: When iterated, the first element will be the video, and the second will be the label as required by the model. Source code in movinets_helper/reader.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def format_features ( video : tf . Tensor , label : str , resolution : int = 172 , scaling_factor : float = 255.0 , num_classes : int = 2 , ) -> Tuple [ tf . Tensor , tf . Tensor ]: \"\"\"Transforms the data to have the appropriate shape. This function must be called on a tf.data.Dataset (passed via its .map method). Args: video (tf.Tensor): Decoded video. label (str): Corresponding class of the video. resolution (int, optional): The resolution will be model dependent. Movinet a0 and a1 use 172, a2 uses 224. Defaults to 172. scaling_factor (float, optional): Given the videos have the pixels in the range 0.255, transforms the data to the range [0, 1]. Defaults to 255.. num_classes (int, optional): Number of classes the model is trained on. I.e. for Kinetics 600 will be 600, for UCF 101 that will be the number. Defaults to 2. Returns: Tuple[tf.Tensor, tf.Tensor]: When iterated, the first element will be the video, and the second will be the label as required by the model. \"\"\" label = tf . cast ( label , tf . int32 ) label = encode_label ( label , num_classes ) video = tf . image . resize ( video , ( resolution , resolution )) video = tf . cast ( video , tf . float32 ) / scaling_factor return video , label","title":"format_features()"},{"location":"api/reader/#movinets_helper.reader.get_dataset","text":"Generates a td.data.Dataset from the TFRecord files. This is the appropriate format to be passed to model.fit, after it is formated and there is some batch called, so the final video object ingested by the model will have the shape [n_videos, n_frames, resolution, resolution, channels]. Parameters: Name Type Description Default filenames List [ str ] List of .tfrecord files. required Returns: Type Description tf . data . Dataset tf.data.Dataset: Dataset ready to train the model. Example target_path is the path to the .tfrecords files directory. ds = get_dataset(list(Path(target_path).iterdir())) This iterable may be formatted appropriately: ds = get_dataset(list(Path(target_path_train).iterdir())) ds = ds.map(format_features) To see a single example: next(iter(ds)) Source code in movinets_helper/reader.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 def get_dataset ( filenames : List [ str ]) -> tf . data . Dataset : \"\"\"Generates a td.data.Dataset from the TFRecord files. This is the appropriate format to be passed to model.fit, after it is formated and there is some batch called, so the final video object ingested by the model will have the shape [n_videos, n_frames, resolution, resolution, channels]. Args: filenames (List[str]): List of .tfrecord files. Returns: tf.data.Dataset: Dataset ready to train the model. Example: target_path is the path to the .tfrecords files directory. >>> ds = get_dataset(list(Path(target_path).iterdir())) This iterable may be formatted appropriately: >>> ds = get_dataset(list(Path(target_path_train).iterdir())) >>> ds = ds.map(format_features) To see a single example: >>> next(iter(ds)) \"\"\" raw_dataset = tf . data . TFRecordDataset ( filenames , compression_type = \"GZIP\" ) return raw_dataset . map ( _parse_example )","title":"get_dataset()"},{"location":"api/utils/","text":"This section contains the documentation of movinets_helper\\utils.py . Help functions. create_class_map ( path ) Given a path to a labels.txt file, creates a class map. Helper function to obtain the classes for the labels. Parameters: Name Type Description Default path str or Path Path pointing to the labels.txt file. required Returns: Type Description Dict [ str , int ] Dict[str, int]: Dict mapping from labels to classes. Source code in movinets_helper/utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def create_class_map ( path : Union [ str , Path ]) -> Dict [ str , int ]: \"\"\"Given a path to a labels.txt file, creates a class map. Helper function to obtain the classes for the labels. Args: path (str or Path): Path pointing to the labels.txt file. Returns: Dict[str, int]: Dict mapping from labels to classes. \"\"\" if isinstance ( path , str ): path = Path ( path ) return { l : i for i , l in enumerate ( path . read_text () . split ( \" \\n \" ))} get_chunks ( l , n ) Yield successive n-sized chunks from l. Used to create n sublists from a list l. copied from: https://github.com/ferreirafabio/video2tfrecord/blob/7aa2c6312e2bc97baed7386b8c92c591769ee5bb/video2tfrecord.py#L55 Parameters: Name Type Description Default l List [ Union [ str , Path ]] required n int required Returns: Type Description Iterable [ Union [ str , Path ]] Iterable[Union[str, Path]]: Example filenames_split = list(get_chunks(filenames, n_videos_in_record)) To obtain the files from a dataframe. next(get_chunks(dataset_df[[\"classes\", \"files\"]].to_records(index=False), 10)) Source code in movinets_helper/utils.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_chunks ( l : List [ Union [ str , Path ]], n : int ) -> Iterable [ Union [ str , Path ]]: r \"\"\"Yield successive n-sized chunks from l. Used to create n sublists from a list l. copied from: https://github.com/ferreirafabio/video2tfrecord/blob/7aa2c6312e2bc97baed7386b8c92c591769ee5bb/video2tfrecord.py#L55 Args: l (List[Union[str, Path]]): n (int): Returns: Iterable[Union[str, Path]]: Example: >>> filenames_split = list(get_chunks(filenames, n_videos_in_record)) To obtain the files from a dataframe. >>> next(get_chunks(dataset_df[[\"classes\", \"files\"]].to_records(index=False), 10)) \"\"\" for i in range ( 0 , len ( l ), n ): yield l [ i : i + n ] get_frame_count ( path ) Counts the total number of frames in a video. Parameters: Name Type Description Default path Path description required Returns: Name Type Description int int description Raises: Type Description AssertionError if the path doesn't exist or it couldn't be loaded by some reason. Example get_video_capture_and_frame_count(sample_clip) 17 Source code in movinets_helper/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_frame_count ( path : Path ) -> int : \"\"\"Counts the total number of frames in a video. Args: path (Path): _description_ Returns: int: _description_ Raises: AssertionError: if the path doesn't exist or it couldn't be loaded by some reason. Example: >>> get_video_capture_and_frame_count(sample_clip) 17 \"\"\" assert path . is_file (), \"Couldn't find video file:\" + path + \". Skipping video.\" cap = None if path : cap = cv2 . VideoCapture ( str ( path )) assert cap is not None , \"Couldn't load video capture:\" + path + \". Skipping video.\" # compute meta data of video if hasattr ( cv2 , \"cv\" ): frame_count = int ( cap . get ( cv2 . cv . CAP_PROP_FRAME_COUNT )) else : frame_count = int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) return frame_count get_label_from_video_name ( filename ) Extracts the label of the movement from the video filename. Examples: >>> path = PosixPath ( '.../chest-to-bar_6.mp4' ) >>> get_label_from_video_name ( path ) \"chest-to-bar\" Source code in movinets_helper/utils.py 89 90 91 92 93 94 95 96 97 98 def get_label_from_video_name ( filename : Path ) -> str : \"\"\"Extracts the label of the movement from the video filename. Examples: >>> path = PosixPath('.../chest-to-bar_6.mp4') >>> get_label_from_video_name(path) \"chest-to-bar\" \"\"\" return filename . stem . split ( \"_\" )[ 0 ] get_number_of_steps ( samples , batch_size , epochs = 1 ) Obtain the number of steps. Computes the number of steps per epoch and the total number of steps to be applied on the LearningScheduler (if used). To be called both for train and validation/test. Parameters: Name Type Description Default samples int Number of examples in the dataset. If the data is splitted in train/test, each of them should be treated independently. required batch_size int Number of videos per step. required epochs int Number of epochs. If bigger than 1, the second returned value corresponds to the total number of steps the network will do. Defaults to 1. 1 Returns: Type Description Tuple [ int , int ] Tuple[int, int]: The first argument will either be the number of steps per epoch or the number of validation steps when calling fit on the model, and the second may be used to estimate the learning rate scheduler (when number of epochs > 1 and computing steps for the training samples). Source code in movinets_helper/utils.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_number_of_steps ( samples : int , batch_size : int , epochs : int = 1 ) -> Tuple [ int , int ]: \"\"\"Obtain the number of steps. Computes the number of steps per epoch and the total number of steps to be applied on the LearningScheduler (if used). To be called both for train and validation/test. Args: samples (int): Number of examples in the dataset. If the data is splitted in train/test, each of them should be treated independently. batch_size (int): Number of videos per step. epochs (int, optional): Number of epochs. If bigger than 1, the second returned value corresponds to the total number of steps the network will do. Defaults to 1. Returns: Tuple[int, int]: The first argument will either be the number of steps per epoch or the number of validation steps when calling `fit` on the model, and the second may be used to estimate the learning rate scheduler (when number of epochs > 1 and computing steps for the training samples). \"\"\" steps = samples // batch_size return steps , steps * epochs get_top_k ( probs , k = 5 , label_map = tf . constant ([ '' ])) Outputs the top k model labels and probabilities on the given video. Source code in movinets_helper/utils.py 225 226 227 228 229 230 231 def get_top_k ( probs , k = 5 , label_map = tf . constant ([ \"\" ])): \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\" top_predictions = tf . argsort ( probs , axis =- 1 , direction = 'DESCENDING' )[: k ] top_labels = tf . gather ( label_map , top_predictions , axis =- 1 ) top_labels = [ label . decode ( 'utf8' ) for label in top_labels . numpy ()] top_probs = tf . gather ( probs , top_predictions , axis =- 1 ) . numpy () return tuple ( zip ( top_labels , top_probs )) load_model ( path_to_model ) Load a model Parameters: Name Type Description Default path_to_model Union [ str , Path ] Path to the SavedModel directory. required Returns: Type Description Functional tf.keras.engine.functional.Functional: Model ready to predict. Source code in movinets_helper/utils.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def load_model ( path_to_model : Union [ str , Path ]) -> \"tf.keras.engine.functional.Functional\" : r \"\"\"Load a model Args: path_to_model (Union[str, Path]): Path to the SavedModel directory. Returns: tf.keras.engine.functional.Functional: Model ready to predict. \"\"\" if isinstance ( path_to_model , Path ): path_to_model = str ( path_to_model ) keras_layer = hub . KerasLayer ( path_to_model ) inputs = tf . keras . layers . Input ( shape = [ None , None , None , 3 ], dtype = tf . float32 ) inputs = dict ( image = inputs ) outputs = keras_layer ( inputs ) model = tf . keras . Model ( inputs , outputs ) model . build ([ 1 , 1 , 1 , 1 , 3 ]) return model load_video_tf ( path ) Loads a video from an mp4 file and returns the decoded tensor. Parameters: Name Type Description Default path str Path to a video. required Returns: Type Description tf . Tensor tf.Tensor: Tensorflow's representation of the video. Source code in movinets_helper/utils.py 14 15 16 17 18 19 20 21 22 23 24 def load_video_tf ( path : str ) -> tf . Tensor : \"\"\"Loads a video from an mp4 file and returns the decoded tensor. Args: path (str): Path to a video. Returns: tf.Tensor: Tensorflow's representation of the video. \"\"\" video = tf . io . read_file ( path ) return tfio . experimental . ffmpeg . decode_video ( video ) parse_video_from_bytes ( video_encoded ) Parses a base64 encoded video to a tf.Tensor. Used to read a video sent through a post request Parameters: Name Type Description Default video_encoded _type_ base64 video encoded. This would be equivalent to: with open(video_path, \"rb\") as video_file: text = base64.b64encode(video_file.read()) video_encoded = base64.b64decode(text) required Returns: Type Description tf . Tensor tf.Tensor: Video as tf.Tensor. Returns the same object that would be obtained from load_video_tf. Source code in movinets_helper/utils.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def parse_video_from_bytes ( video_encoded : bytes ) -> tf . Tensor : \"\"\"Parses a base64 encoded video to a tf.Tensor. Used to read a video sent through a post request Args: video_encoded (_type_): base64 video encoded. This would be equivalent to: >>> with open(video_path, \"rb\") as video_file: >>> text = base64.b64encode(video_file.read()) >>> video_encoded = base64.b64decode(text) Returns: tf.Tensor: Video as tf.Tensor. Returns the same object that would be obtained from load_video_tf. \"\"\" return tfio . experimental . ffmpeg . decode_video ( base64 . b64decode ( video_encoded )) predict_top_k ( model , video , k = 5 , label_map = tf . constant ([ '' ])) Outputs the top k model labels and probabilities on the given video. Parameters: Name Type Description Default model _type_ description required video _type_ description required k int description . Defaults to 5. 5 label_map _type_ description . Defaults to tf.constant([\"\"]). tf.constant(['']) Returns: Name Type Description _type_ description Source code in movinets_helper/utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def predict_top_k ( model , video , k = 5 , label_map = tf . constant ([ \"\" ])): \"\"\"Outputs the top k model labels and probabilities on the given video. Args: model (_type_): _description_ video (_type_): _description_ k (int, optional): _description_. Defaults to 5. label_map (_type_, optional): _description_. Defaults to tf.constant([\"\"]). Returns: _type_: _description_ \"\"\" outputs = model . predict ( video [ tf . newaxis ])[ 0 ] probs = tf . nn . softmax ( outputs ) return get_top_k ( probs , k = k , label_map = label_map ) prepare_to_predict ( video , resolution = 224 ) Adds a dimension to a video (and possibly resizes to the wanted resolution) The model expects the videos with 5 dimensions. When a video was loaded with load_video_tf it will have only the 4 dimension expected, which would represent the batch size during training. Parameters: Name Type Description Default video tf . Tensor video as tf.Tensor, maybe loaded with load_video_tf. required resolution int If is different from None, resizes the video, to the expected resolution of the model. If set to None, doesn't do anything. Defaults to 224 (expected shape for a2 model). 224 Returns: Type Description tf . Tensor tf.Tensor: video ready to be passed to model.predict(video) Source code in movinets_helper/utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def prepare_to_predict ( video : tf . Tensor , resolution : int = 224 ) -> tf . Tensor : \"\"\"Adds a dimension to a video (and possibly resizes to the wanted resolution) The model expects the videos with 5 dimensions. When a video was loaded with load_video_tf it will have only the 4 dimension expected, which would represent the batch size during training. Args: video (tf.Tensor): video as tf.Tensor, maybe loaded with load_video_tf. resolution (int): If is different from None, resizes the video, to the expected resolution of the model. If set to None, doesn't do anything. Defaults to 224 (expected shape for a2 model). Returns: tf.Tensor: video ready to be passed to model.predict(video) \"\"\" if resolution is not None : video = tf . image . resize ( video , [ resolution , resolution ]) return tf . expand_dims ( video , 0 ) split_train_test ( dataset , train_size = 0.8 , seed = 5678 ) Simple function to split a dataset in train/test. This functionality may be obtained from many other libraries, its just here for personal convinience. Parameters: Name Type Description Default dataset pd . DataFrame DataFrame with 3 columns: labels, files and classes. required train_size float Percentage of the sample for training, range [0, 1]. 0.8 seed int Random seed to split the data. Defaults to 5678. 5678 Returns: Type Description Tuple [ pd . DataFrame , pd . DataFrame ] Tuple[pd.DataFrame, pd.DataFrame]: tran and test datasets. Source code in movinets_helper/utils.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def split_train_test ( dataset : pd . DataFrame , train_size : float = 0.8 , seed : int = 5678 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Simple function to split a dataset in train/test. This functionality may be obtained from many other libraries, its just here for personal convinience. Args: dataset (pd.DataFrame): DataFrame with 3 columns: labels, files and classes. train_size (float): Percentage of the sample for training, range [0, 1]. seed (int, optional): Random seed to split the data. Defaults to 5678. Returns: Tuple[pd.DataFrame, pd.DataFrame]: tran and test datasets. \"\"\" train = dataset . sample ( int ( len ( dataset ) * train_size ), random_state = seed ) test = dataset . loc [ set ( dataset . index ) . difference ( train . index ), :] return train , test","title":"utils"},{"location":"api/utils/#movinets_helper.utils.create_class_map","text":"Given a path to a labels.txt file, creates a class map. Helper function to obtain the classes for the labels. Parameters: Name Type Description Default path str or Path Path pointing to the labels.txt file. required Returns: Type Description Dict [ str , int ] Dict[str, int]: Dict mapping from labels to classes. Source code in movinets_helper/utils.py 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def create_class_map ( path : Union [ str , Path ]) -> Dict [ str , int ]: \"\"\"Given a path to a labels.txt file, creates a class map. Helper function to obtain the classes for the labels. Args: path (str or Path): Path pointing to the labels.txt file. Returns: Dict[str, int]: Dict mapping from labels to classes. \"\"\" if isinstance ( path , str ): path = Path ( path ) return { l : i for i , l in enumerate ( path . read_text () . split ( \" \\n \" ))}","title":"create_class_map()"},{"location":"api/utils/#movinets_helper.utils.get_chunks","text":"Yield successive n-sized chunks from l. Used to create n sublists from a list l. copied from: https://github.com/ferreirafabio/video2tfrecord/blob/7aa2c6312e2bc97baed7386b8c92c591769ee5bb/video2tfrecord.py#L55 Parameters: Name Type Description Default l List [ Union [ str , Path ]] required n int required Returns: Type Description Iterable [ Union [ str , Path ]] Iterable[Union[str, Path]]: Example filenames_split = list(get_chunks(filenames, n_videos_in_record)) To obtain the files from a dataframe. next(get_chunks(dataset_df[[\"classes\", \"files\"]].to_records(index=False), 10)) Source code in movinets_helper/utils.py 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 def get_chunks ( l : List [ Union [ str , Path ]], n : int ) -> Iterable [ Union [ str , Path ]]: r \"\"\"Yield successive n-sized chunks from l. Used to create n sublists from a list l. copied from: https://github.com/ferreirafabio/video2tfrecord/blob/7aa2c6312e2bc97baed7386b8c92c591769ee5bb/video2tfrecord.py#L55 Args: l (List[Union[str, Path]]): n (int): Returns: Iterable[Union[str, Path]]: Example: >>> filenames_split = list(get_chunks(filenames, n_videos_in_record)) To obtain the files from a dataframe. >>> next(get_chunks(dataset_df[[\"classes\", \"files\"]].to_records(index=False), 10)) \"\"\" for i in range ( 0 , len ( l ), n ): yield l [ i : i + n ]","title":"get_chunks()"},{"location":"api/utils/#movinets_helper.utils.get_frame_count","text":"Counts the total number of frames in a video. Parameters: Name Type Description Default path Path description required Returns: Name Type Description int int description Raises: Type Description AssertionError if the path doesn't exist or it couldn't be loaded by some reason. Example get_video_capture_and_frame_count(sample_clip) 17 Source code in movinets_helper/utils.py 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def get_frame_count ( path : Path ) -> int : \"\"\"Counts the total number of frames in a video. Args: path (Path): _description_ Returns: int: _description_ Raises: AssertionError: if the path doesn't exist or it couldn't be loaded by some reason. Example: >>> get_video_capture_and_frame_count(sample_clip) 17 \"\"\" assert path . is_file (), \"Couldn't find video file:\" + path + \". Skipping video.\" cap = None if path : cap = cv2 . VideoCapture ( str ( path )) assert cap is not None , \"Couldn't load video capture:\" + path + \". Skipping video.\" # compute meta data of video if hasattr ( cv2 , \"cv\" ): frame_count = int ( cap . get ( cv2 . cv . CAP_PROP_FRAME_COUNT )) else : frame_count = int ( cap . get ( cv2 . CAP_PROP_FRAME_COUNT )) return frame_count","title":"get_frame_count()"},{"location":"api/utils/#movinets_helper.utils.get_label_from_video_name","text":"Extracts the label of the movement from the video filename. Examples: >>> path = PosixPath ( '.../chest-to-bar_6.mp4' ) >>> get_label_from_video_name ( path ) \"chest-to-bar\" Source code in movinets_helper/utils.py 89 90 91 92 93 94 95 96 97 98 def get_label_from_video_name ( filename : Path ) -> str : \"\"\"Extracts the label of the movement from the video filename. Examples: >>> path = PosixPath('.../chest-to-bar_6.mp4') >>> get_label_from_video_name(path) \"chest-to-bar\" \"\"\" return filename . stem . split ( \"_\" )[ 0 ]","title":"get_label_from_video_name()"},{"location":"api/utils/#movinets_helper.utils.get_number_of_steps","text":"Obtain the number of steps. Computes the number of steps per epoch and the total number of steps to be applied on the LearningScheduler (if used). To be called both for train and validation/test. Parameters: Name Type Description Default samples int Number of examples in the dataset. If the data is splitted in train/test, each of them should be treated independently. required batch_size int Number of videos per step. required epochs int Number of epochs. If bigger than 1, the second returned value corresponds to the total number of steps the network will do. Defaults to 1. 1 Returns: Type Description Tuple [ int , int ] Tuple[int, int]: The first argument will either be the number of steps per epoch or the number of validation steps when calling fit on the model, and the second may be used to estimate the learning rate scheduler (when number of epochs > 1 and computing steps for the training samples). Source code in movinets_helper/utils.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 def get_number_of_steps ( samples : int , batch_size : int , epochs : int = 1 ) -> Tuple [ int , int ]: \"\"\"Obtain the number of steps. Computes the number of steps per epoch and the total number of steps to be applied on the LearningScheduler (if used). To be called both for train and validation/test. Args: samples (int): Number of examples in the dataset. If the data is splitted in train/test, each of them should be treated independently. batch_size (int): Number of videos per step. epochs (int, optional): Number of epochs. If bigger than 1, the second returned value corresponds to the total number of steps the network will do. Defaults to 1. Returns: Tuple[int, int]: The first argument will either be the number of steps per epoch or the number of validation steps when calling `fit` on the model, and the second may be used to estimate the learning rate scheduler (when number of epochs > 1 and computing steps for the training samples). \"\"\" steps = samples // batch_size return steps , steps * epochs","title":"get_number_of_steps()"},{"location":"api/utils/#movinets_helper.utils.get_top_k","text":"Outputs the top k model labels and probabilities on the given video. Source code in movinets_helper/utils.py 225 226 227 228 229 230 231 def get_top_k ( probs , k = 5 , label_map = tf . constant ([ \"\" ])): \"\"\"Outputs the top k model labels and probabilities on the given video.\"\"\" top_predictions = tf . argsort ( probs , axis =- 1 , direction = 'DESCENDING' )[: k ] top_labels = tf . gather ( label_map , top_predictions , axis =- 1 ) top_labels = [ label . decode ( 'utf8' ) for label in top_labels . numpy ()] top_probs = tf . gather ( probs , top_predictions , axis =- 1 ) . numpy () return tuple ( zip ( top_labels , top_probs ))","title":"get_top_k()"},{"location":"api/utils/#movinets_helper.utils.load_model","text":"Load a model Parameters: Name Type Description Default path_to_model Union [ str , Path ] Path to the SavedModel directory. required Returns: Type Description Functional tf.keras.engine.functional.Functional: Model ready to predict. Source code in movinets_helper/utils.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 def load_model ( path_to_model : Union [ str , Path ]) -> \"tf.keras.engine.functional.Functional\" : r \"\"\"Load a model Args: path_to_model (Union[str, Path]): Path to the SavedModel directory. Returns: tf.keras.engine.functional.Functional: Model ready to predict. \"\"\" if isinstance ( path_to_model , Path ): path_to_model = str ( path_to_model ) keras_layer = hub . KerasLayer ( path_to_model ) inputs = tf . keras . layers . Input ( shape = [ None , None , None , 3 ], dtype = tf . float32 ) inputs = dict ( image = inputs ) outputs = keras_layer ( inputs ) model = tf . keras . Model ( inputs , outputs ) model . build ([ 1 , 1 , 1 , 1 , 3 ]) return model","title":"load_model()"},{"location":"api/utils/#movinets_helper.utils.load_video_tf","text":"Loads a video from an mp4 file and returns the decoded tensor. Parameters: Name Type Description Default path str Path to a video. required Returns: Type Description tf . Tensor tf.Tensor: Tensorflow's representation of the video. Source code in movinets_helper/utils.py 14 15 16 17 18 19 20 21 22 23 24 def load_video_tf ( path : str ) -> tf . Tensor : \"\"\"Loads a video from an mp4 file and returns the decoded tensor. Args: path (str): Path to a video. Returns: tf.Tensor: Tensorflow's representation of the video. \"\"\" video = tf . io . read_file ( path ) return tfio . experimental . ffmpeg . decode_video ( video )","title":"load_video_tf()"},{"location":"api/utils/#movinets_helper.utils.parse_video_from_bytes","text":"Parses a base64 encoded video to a tf.Tensor. Used to read a video sent through a post request Parameters: Name Type Description Default video_encoded _type_ base64 video encoded. This would be equivalent to: with open(video_path, \"rb\") as video_file: text = base64.b64encode(video_file.read()) video_encoded = base64.b64decode(text) required Returns: Type Description tf . Tensor tf.Tensor: Video as tf.Tensor. Returns the same object that would be obtained from load_video_tf. Source code in movinets_helper/utils.py 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def parse_video_from_bytes ( video_encoded : bytes ) -> tf . Tensor : \"\"\"Parses a base64 encoded video to a tf.Tensor. Used to read a video sent through a post request Args: video_encoded (_type_): base64 video encoded. This would be equivalent to: >>> with open(video_path, \"rb\") as video_file: >>> text = base64.b64encode(video_file.read()) >>> video_encoded = base64.b64decode(text) Returns: tf.Tensor: Video as tf.Tensor. Returns the same object that would be obtained from load_video_tf. \"\"\" return tfio . experimental . ffmpeg . decode_video ( base64 . b64decode ( video_encoded ))","title":"parse_video_from_bytes()"},{"location":"api/utils/#movinets_helper.utils.predict_top_k","text":"Outputs the top k model labels and probabilities on the given video. Parameters: Name Type Description Default model _type_ description required video _type_ description required k int description . Defaults to 5. 5 label_map _type_ description . Defaults to tf.constant([\"\"]). tf.constant(['']) Returns: Name Type Description _type_ description Source code in movinets_helper/utils.py 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 def predict_top_k ( model , video , k = 5 , label_map = tf . constant ([ \"\" ])): \"\"\"Outputs the top k model labels and probabilities on the given video. Args: model (_type_): _description_ video (_type_): _description_ k (int, optional): _description_. Defaults to 5. label_map (_type_, optional): _description_. Defaults to tf.constant([\"\"]). Returns: _type_: _description_ \"\"\" outputs = model . predict ( video [ tf . newaxis ])[ 0 ] probs = tf . nn . softmax ( outputs ) return get_top_k ( probs , k = k , label_map = label_map )","title":"predict_top_k()"},{"location":"api/utils/#movinets_helper.utils.prepare_to_predict","text":"Adds a dimension to a video (and possibly resizes to the wanted resolution) The model expects the videos with 5 dimensions. When a video was loaded with load_video_tf it will have only the 4 dimension expected, which would represent the batch size during training. Parameters: Name Type Description Default video tf . Tensor video as tf.Tensor, maybe loaded with load_video_tf. required resolution int If is different from None, resizes the video, to the expected resolution of the model. If set to None, doesn't do anything. Defaults to 224 (expected shape for a2 model). 224 Returns: Type Description tf . Tensor tf.Tensor: video ready to be passed to model.predict(video) Source code in movinets_helper/utils.py 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 def prepare_to_predict ( video : tf . Tensor , resolution : int = 224 ) -> tf . Tensor : \"\"\"Adds a dimension to a video (and possibly resizes to the wanted resolution) The model expects the videos with 5 dimensions. When a video was loaded with load_video_tf it will have only the 4 dimension expected, which would represent the batch size during training. Args: video (tf.Tensor): video as tf.Tensor, maybe loaded with load_video_tf. resolution (int): If is different from None, resizes the video, to the expected resolution of the model. If set to None, doesn't do anything. Defaults to 224 (expected shape for a2 model). Returns: tf.Tensor: video ready to be passed to model.predict(video) \"\"\" if resolution is not None : video = tf . image . resize ( video , [ resolution , resolution ]) return tf . expand_dims ( video , 0 )","title":"prepare_to_predict()"},{"location":"api/utils/#movinets_helper.utils.split_train_test","text":"Simple function to split a dataset in train/test. This functionality may be obtained from many other libraries, its just here for personal convinience. Parameters: Name Type Description Default dataset pd . DataFrame DataFrame with 3 columns: labels, files and classes. required train_size float Percentage of the sample for training, range [0, 1]. 0.8 seed int Random seed to split the data. Defaults to 5678. 5678 Returns: Type Description Tuple [ pd . DataFrame , pd . DataFrame ] Tuple[pd.DataFrame, pd.DataFrame]: tran and test datasets. Source code in movinets_helper/utils.py 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 def split_train_test ( dataset : pd . DataFrame , train_size : float = 0.8 , seed : int = 5678 ) -> Tuple [ pd . DataFrame , pd . DataFrame ]: \"\"\"Simple function to split a dataset in train/test. This functionality may be obtained from many other libraries, its just here for personal convinience. Args: dataset (pd.DataFrame): DataFrame with 3 columns: labels, files and classes. train_size (float): Percentage of the sample for training, range [0, 1]. seed (int, optional): Random seed to split the data. Defaults to 5678. Returns: Tuple[pd.DataFrame, pd.DataFrame]: tran and test datasets. \"\"\" train = dataset . sample ( int ( len ( dataset ) * train_size ), random_state = seed ) test = dataset . loc [ set ( dataset . index ) . difference ( train . index ), :] return train , test","title":"split_train_test()"},{"location":"api/writer/","text":"This section contains the documentation of movinets_helper\\writer.py . Helper to write mp4 videos to TFRecordDataset. convert_mp4_to_tfrecord ( dataset_df , target_path , n_videos_in_record = 15 , n_frames_per_video = 10 , resolution = 224 ) Saves videos in mp4 format to tfrecords. It creates a new dataset of homogeneus videos, all with shape: (n_frames, resolution, resolution, channels=3), along with its label and shape features to be loaded back. Parameters: Name Type Description Default dataset_df pd . DataFrame Dataframe with 2 columns: classes, representing the labels, and files, with the path to the videos. required target_path Path Path where the files will be generated. It must be already generated. required n_videos_in_record int Number of videos per file. See the notes in https://www.tensorflow.org/tutorials/load_data/tfrecord Defaults to 10. 15 n_frames_per_video int Number of frames to select per video. Defaults to 10. 10 resolution int Try to keep to as low as possible to save space and computing time during writing. This can be updated later. Defaults to 224. 224 Example Generate a dataset of tf records convert_mp4_to_tfrecord(dataset_df[[\"classes\", \"files\"]], target_path) Split into train/test convert_mp4_to_tfrecord(train_dataset_df[[\"classes\", \"files\"]], target_path_train) convert_mp4_to_tfrecord(test_dataset_df[[\"classes\", \"files\"]], target_path_test) Note This data format isn't storage friendly, even after compression. Source code in movinets_helper/writer.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def convert_mp4_to_tfrecord ( dataset_df : pd . DataFrame , target_path : Path , n_videos_in_record : int = 15 , n_frames_per_video : int = 10 , resolution : int = 224 , ): r \"\"\"Saves videos in mp4 format to tfrecords. It creates a new dataset of homogeneus videos, all with shape: (n_frames, resolution, resolution, channels=3), along with its label and shape features to be loaded back. Args: dataset_df (pd.DataFrame): Dataframe with 2 columns: classes, representing the labels, and files, with the path to the videos. target_path (Path): Path where the files will be generated. It must be already generated. n_videos_in_record (int, optional): Number of videos per file. See the notes in https://www.tensorflow.org/tutorials/load_data/tfrecord Defaults to 10. n_frames_per_video (int, optional): Number of frames to select per video. Defaults to 10. resolution (int, optional): Try to keep to as low as possible to save space and computing time during writing. This can be updated later. Defaults to 224. Example: Generate a dataset of tf records >>> convert_mp4_to_tfrecord(dataset_df[[\"classes\", \"files\"]], target_path) Split into train/test >>> convert_mp4_to_tfrecord(train_dataset_df[[\"classes\", \"files\"]], target_path_train) >>> convert_mp4_to_tfrecord(test_dataset_df[[\"classes\", \"files\"]], target_path_test) Note: This data format isn't storage friendly, even after compression. \"\"\" if not Path ( target_path ) . is_dir (): print ( f \" { target_path } doesn't exist, create it first.\" ) return assert dataset_df . columns . tolist () == [ \"classes\" , \"files\" , ], \"dataset_df doesn't have the expected columns\" files = dataset_df . to_records ( index = False ) # List of tuples (label, filename) filenames_split = list ( get_chunks ( files , n_videos_in_record )) total_batches = len ( filenames_split ) for i , batch in enumerate ( tqdm . tqdm ( filenames_split )): tfrecord_filename = str ( Path ( target_path ) / f \"video_batch_ { i } _ { total_batches } .tfrecords\" ) with tf . io . TFRecordWriter ( tfrecord_filename , options = tf . io . TFRecordOptions ( compression_type = \"GZIP\" ) ) as file_writer : for label , file in batch : try : video = load_video_tf ( file ) except tf . errors . InvalidArgumentError : print ( f \"Video without length, passing\" ) continue video = tf . image . resize ( video , ( resolution , resolution )) frame_count = get_frame_count ( Path ( file )) video = _select_frames ( video , frame_count , n_frames_per_video ) try : file_writer . write ( serialize_example ( label , video )) except Exception as exc : print ( f \"Couldn't serialize file: { file } \" ) print ( f \"err: { exc } \" ) serialize_example ( label , video ) Creates a tf.train.Example message ready to be written to a file. The output can be sent to a TFRecordWriter to be written. Parameters: Name Type Description Default label tf . Tensor int encoded label. required video tf . Tensor video as a tensor, maybe loaded using load_video_tf. required Returns: Name Type Description str str representation of the features to be stored as .tfrecords. Note Depending on the size (i.e., videos), jupyter won't be able to show them, a javascript error will raise. Source code in movinets_helper/writer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def serialize_example ( label : tf . Tensor , video : tf . Tensor ) -> str : \"\"\"Creates a tf.train.Example message ready to be written to a file. The output can be sent to a TFRecordWriter to be written. Args: label (tf.Tensor): int encoded label. video (tf.Tensor): video as a tensor, maybe loaded using load_video_tf. Returns: str: representation of the features to be stored as .tfrecords. Note: Depending on the size (i.e., videos), jupyter won't be able to show them, a javascript error will raise. \"\"\" # dict mapping the feature name to the tf.train.Example-compatible # data type. features = { \"label\" : _int64_feature ( label ), \"n_frames\" : _int64_feature ( video . shape [ 0 ]), \"width\" : _int64_feature ( video . shape [ 1 ]), \"height\" : _int64_feature ( video . shape [ 2 ]), \"n_channels\" : _int64_feature ( video . shape [ 3 ]), \"video\" : _bytes_feature ( tf . io . serialize_tensor ( video . numpy ())), } # Create a Features message using tf.train.Example. example_proto = tf . train . Example ( features = tf . train . Features ( feature = features )) return example_proto . SerializeToString ()","title":"writer"},{"location":"api/writer/#movinets_helper.writer.convert_mp4_to_tfrecord","text":"Saves videos in mp4 format to tfrecords. It creates a new dataset of homogeneus videos, all with shape: (n_frames, resolution, resolution, channels=3), along with its label and shape features to be loaded back. Parameters: Name Type Description Default dataset_df pd . DataFrame Dataframe with 2 columns: classes, representing the labels, and files, with the path to the videos. required target_path Path Path where the files will be generated. It must be already generated. required n_videos_in_record int Number of videos per file. See the notes in https://www.tensorflow.org/tutorials/load_data/tfrecord Defaults to 10. 15 n_frames_per_video int Number of frames to select per video. Defaults to 10. 10 resolution int Try to keep to as low as possible to save space and computing time during writing. This can be updated later. Defaults to 224. 224 Example Generate a dataset of tf records convert_mp4_to_tfrecord(dataset_df[[\"classes\", \"files\"]], target_path) Split into train/test convert_mp4_to_tfrecord(train_dataset_df[[\"classes\", \"files\"]], target_path_train) convert_mp4_to_tfrecord(test_dataset_df[[\"classes\", \"files\"]], target_path_test) Note This data format isn't storage friendly, even after compression. Source code in movinets_helper/writer.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def convert_mp4_to_tfrecord ( dataset_df : pd . DataFrame , target_path : Path , n_videos_in_record : int = 15 , n_frames_per_video : int = 10 , resolution : int = 224 , ): r \"\"\"Saves videos in mp4 format to tfrecords. It creates a new dataset of homogeneus videos, all with shape: (n_frames, resolution, resolution, channels=3), along with its label and shape features to be loaded back. Args: dataset_df (pd.DataFrame): Dataframe with 2 columns: classes, representing the labels, and files, with the path to the videos. target_path (Path): Path where the files will be generated. It must be already generated. n_videos_in_record (int, optional): Number of videos per file. See the notes in https://www.tensorflow.org/tutorials/load_data/tfrecord Defaults to 10. n_frames_per_video (int, optional): Number of frames to select per video. Defaults to 10. resolution (int, optional): Try to keep to as low as possible to save space and computing time during writing. This can be updated later. Defaults to 224. Example: Generate a dataset of tf records >>> convert_mp4_to_tfrecord(dataset_df[[\"classes\", \"files\"]], target_path) Split into train/test >>> convert_mp4_to_tfrecord(train_dataset_df[[\"classes\", \"files\"]], target_path_train) >>> convert_mp4_to_tfrecord(test_dataset_df[[\"classes\", \"files\"]], target_path_test) Note: This data format isn't storage friendly, even after compression. \"\"\" if not Path ( target_path ) . is_dir (): print ( f \" { target_path } doesn't exist, create it first.\" ) return assert dataset_df . columns . tolist () == [ \"classes\" , \"files\" , ], \"dataset_df doesn't have the expected columns\" files = dataset_df . to_records ( index = False ) # List of tuples (label, filename) filenames_split = list ( get_chunks ( files , n_videos_in_record )) total_batches = len ( filenames_split ) for i , batch in enumerate ( tqdm . tqdm ( filenames_split )): tfrecord_filename = str ( Path ( target_path ) / f \"video_batch_ { i } _ { total_batches } .tfrecords\" ) with tf . io . TFRecordWriter ( tfrecord_filename , options = tf . io . TFRecordOptions ( compression_type = \"GZIP\" ) ) as file_writer : for label , file in batch : try : video = load_video_tf ( file ) except tf . errors . InvalidArgumentError : print ( f \"Video without length, passing\" ) continue video = tf . image . resize ( video , ( resolution , resolution )) frame_count = get_frame_count ( Path ( file )) video = _select_frames ( video , frame_count , n_frames_per_video ) try : file_writer . write ( serialize_example ( label , video )) except Exception as exc : print ( f \"Couldn't serialize file: { file } \" ) print ( f \"err: { exc } \" )","title":"convert_mp4_to_tfrecord()"},{"location":"api/writer/#movinets_helper.writer.serialize_example","text":"Creates a tf.train.Example message ready to be written to a file. The output can be sent to a TFRecordWriter to be written. Parameters: Name Type Description Default label tf . Tensor int encoded label. required video tf . Tensor video as a tensor, maybe loaded using load_video_tf. required Returns: Name Type Description str str representation of the features to be stored as .tfrecords. Note Depending on the size (i.e., videos), jupyter won't be able to show them, a javascript error will raise. Source code in movinets_helper/writer.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def serialize_example ( label : tf . Tensor , video : tf . Tensor ) -> str : \"\"\"Creates a tf.train.Example message ready to be written to a file. The output can be sent to a TFRecordWriter to be written. Args: label (tf.Tensor): int encoded label. video (tf.Tensor): video as a tensor, maybe loaded using load_video_tf. Returns: str: representation of the features to be stored as .tfrecords. Note: Depending on the size (i.e., videos), jupyter won't be able to show them, a javascript error will raise. \"\"\" # dict mapping the feature name to the tf.train.Example-compatible # data type. features = { \"label\" : _int64_feature ( label ), \"n_frames\" : _int64_feature ( video . shape [ 0 ]), \"width\" : _int64_feature ( video . shape [ 1 ]), \"height\" : _int64_feature ( video . shape [ 2 ]), \"n_channels\" : _int64_feature ( video . shape [ 3 ]), \"video\" : _bytes_feature ( tf . io . serialize_tensor ( video . numpy ())), } # Create a Features message using tf.train.Example. example_proto = tf . train . Example ( features = tf . train . Features ( feature = features )) return example_proto . SerializeToString ()","title":"serialize_example()"}]}